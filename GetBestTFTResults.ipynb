{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65342ed8",
   "metadata": {},
   "source": [
    "# TFT Hyperparameter Tuning Results Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of all TFT (Temporal Fusion Transformer) experiments run using Optuna for Henry Hub natural gas price prediction.\n",
    "\n",
    "## Contents\n",
    "1. **Experiment Overview** ‚Äî Summary statistics for each experiment\n",
    "2. **Best Model Comparison** ‚Äî Metrics comparison across experiments\n",
    "3. **Pruning Analysis** ‚Äî Pruning rates and efficiency per experiment\n",
    "4. **Hyperparameter Analysis** ‚Äî Distributions and impact of parameters\n",
    "5. **Cross-Experiment Parameter Comparison** ‚Äî What works best overall\n",
    "6. **Visualizations** ‚Äî Charts and graphs for all comparisons\n",
    "\n",
    "### Experiments Analysed:\n",
    "- **Price Only** ‚Äî Univariate model using only historical prices\n",
    "- **Price + Storage** ‚Äî Prices with gas storage levels\n",
    "- **Price + Production** ‚Äî Prices with US dry gas production\n",
    "- **Price + USD** ‚Äî Prices with USD index\n",
    "- **Price + Weather** ‚Äî Prices with HDD/CDD weather data\n",
    "- **All Features** ‚Äî All features combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Setup and Imports ===\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Color palette for experiments\n",
    "EXPERIMENT_COLORS = {\n",
    "    \"Price Only\": \"#1f77b4\",\n",
    "    \"Price + Storage\": \"#ff7f0e\", \n",
    "    \"Price + Production\": \"#2ca02c\",\n",
    "    \"Price + USD\": \"#d62728\",\n",
    "    \"Price + Weather\": \"#9467bd\",\n",
    "    \"All Features\": \"#8c564b\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92acad52",
   "metadata": {},
   "source": [
    "## 1. Load All Experiment Data\n",
    "\n",
    "Define paths to each experiment's trial summary CSV and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Define Experiment Paths and Load Data ===\n",
    "\n",
    "SAVED_RESULTS_DIR = Path(\"saved_results\")\n",
    "\n",
    "# Auto-discover TFT experiment folders\n",
    "# Pattern: <timestamp>_<experimentname>_tft\n",
    "def discover_tft_experiments(base_dir: Path) -> Dict[str, str]:\n",
    "    \"\"\"Automatically discover TFT experiment folders.\"\"\"\n",
    "    experiments = {}\n",
    "    if not base_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è Directory not found: {base_dir}\")\n",
    "        return experiments\n",
    "    \n",
    "    # Map folder patterns to experiment names\n",
    "    name_mapping = {\n",
    "        \"PriceOnly_tft\": \"Price Only\",\n",
    "        \"Price+Storage_tft\": \"Price + Storage\",\n",
    "        \"Price+Production_tft\": \"Price + Production\",\n",
    "        \"Price+USD_tft\": \"Price + USD\",\n",
    "        \"Price+Weather_tft\": \"Price + Weather\",\n",
    "        \"AllFeatures_tft\": \"All Features\",\n",
    "    }\n",
    "    \n",
    "    for folder in sorted(base_dir.iterdir()):\n",
    "        if folder.is_dir() and \"_tft\" in folder.name:\n",
    "            # Try to match with known experiment names\n",
    "            for pattern, name in name_mapping.items():\n",
    "                if pattern in folder.name:\n",
    "                    # If multiple runs exist, keep the latest (highest timestamp)\n",
    "                    if name not in experiments or folder.name > experiments[name]:\n",
    "                        experiments[name] = folder.name\n",
    "                    break\n",
    "    \n",
    "    return experiments\n",
    "\n",
    "# Discover experiments\n",
    "EXPERIMENTS = discover_tft_experiments(SAVED_RESULTS_DIR)\n",
    "print(f\"Discovered TFT experiments: {EXPERIMENTS}\")\n",
    "\n",
    "# If no experiments found, you can manually specify:\n",
    "if not EXPERIMENTS:\n",
    "    print(\"\\n‚ö†Ô∏è No TFT experiments discovered. Please specify manually:\")\n",
    "    print(\"\"\"EXPERIMENTS = {\n",
    "    \"Price Only\": \"YYYYMMDD-HHMMSS_PriceOnly_tft\",\n",
    "    \"Price + Storage\": \"YYYYMMDD-HHMMSS_Price+Storage_tft\",\n",
    "    ....\n",
    "}\"\"\")\n",
    "\n",
    "def load_trial_summary(experiment_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load trial_summary.csv and parse the params JSON column.\"\"\"\n",
    "    csv_path = experiment_dir / \"trial_summary.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Not found: {csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Parse params column from JSON string to dict\n",
    "    if 'params' in df.columns:\n",
    "        df['params_dict'] = df['params'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else {})\n",
    "        \n",
    "        # Extract individual TFT hyperparameters\n",
    "        df['lookback'] = df['params_dict'].apply(lambda x: x.get('max_encoder_length'))\n",
    "        df['batch_size'] = df['params_dict'].apply(lambda x: x.get('batch_size'))\n",
    "        df['hidden_size'] = df['params_dict'].apply(lambda x: x.get('hidden_size'))\n",
    "        df['attention_head_size'] = df['params_dict'].apply(lambda x: x.get('attention_head_size'))\n",
    "        df['hidden_continuous_size'] = df['params_dict'].apply(lambda x: x.get('hidden_continuous_size'))\n",
    "        df['dropout'] = df['params_dict'].apply(lambda x: x.get('dropout'))\n",
    "        df['lstm_layers'] = df['params_dict'].apply(lambda x: x.get('lstm_layers'))\n",
    "        df['learning_rate'] = df['params_dict'].apply(lambda x: x.get('learning_rate'))\n",
    "        df['gradient_clip_val'] = df['params_dict'].apply(lambda x: x.get('gradient_clip_val'))\n",
    "        df['weight_decay'] = df['params_dict'].apply(lambda x: x.get('weight_decay'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all experiments\n",
    "experiment_data = {}\n",
    "for name, folder in EXPERIMENTS.items():\n",
    "    exp_path = SAVED_RESULTS_DIR / folder\n",
    "    df = load_trial_summary(exp_path)\n",
    "    if df is not None:\n",
    "        experiment_data[name] = df\n",
    "        print(f\"‚úÖ Loaded {name}: {len(df)} trials\")\n",
    "\n",
    "print(f\"\\nüìä Total experiments loaded: {len(experiment_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd44c94",
   "metadata": {},
   "source": [
    "## 2. Experiment Overview & Summary Statistics\n",
    "\n",
    "Summary of each experiment including total trials, completion rates, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ac9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Generate Experiment Summary Statistics ===\n",
    "\n",
    "def get_experiment_summary(df: pd.DataFrame, name: str) -> dict:\n",
    "    \"\"\"Extract summary statistics for an experiment.\"\"\"\n",
    "    completed = df[df['state'] == 'COMPLETE']\n",
    "    pruned = df[df['state'] == 'PRUNED']\n",
    "    \n",
    "    summary = {\n",
    "        'Experiment': name,\n",
    "        'Total Trials': len(df),\n",
    "        'Completed': len(completed),\n",
    "        'Pruned': len(pruned),\n",
    "        'Completion Rate': f\"{len(completed)/len(df)*100:.1f}%\" if len(df) > 0 else \"N/A\",\n",
    "        'Pruning Rate': f\"{len(pruned)/len(df)*100:.1f}%\" if len(df) > 0 else \"N/A\",\n",
    "        'Best Val Loss': f\"{completed['value'].min():.6f}\" if len(completed) > 0 else \"N/A\",\n",
    "        'Mean Val Loss': f\"{completed['value'].mean():.6f}\" if len(completed) > 0 else \"N/A\",\n",
    "        'Std Val Loss': f\"{completed['value'].std():.6f}\" if len(completed) > 0 else \"N/A\"\n",
    "    }\n",
    "    \n",
    "    # Add test metrics if available\n",
    "    if 'test_mae' in completed.columns:\n",
    "        best_idx = completed['value'].idxmin()\n",
    "        summary['Best Test MAE'] = f\"{completed.loc[best_idx, 'test_mae']:.4f}\" if pd.notna(completed.loc[best_idx, 'test_mae']) else \"N/A\"\n",
    "        summary['Best Test RMSE'] = f\"{completed.loc[best_idx, 'test_rmse']:.4f}\" if pd.notna(completed.loc[best_idx, 'test_rmse']) else \"N/A\"\n",
    "        summary['Best Test MAPE'] = f\"{completed.loc[best_idx, 'test_mape']:.2f}%\" if pd.notna(completed.loc[best_idx, 'test_mape']) else \"N/A\"\n",
    "        summary['Best Dir Acc'] = f\"{completed.loc[best_idx, 'test_directional_accuracy']:.2f}%\" if pd.notna(completed.loc[best_idx, 'test_directional_accuracy']) else \"N/A\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary table\n",
    "summaries = []\n",
    "for name, df in experiment_data.items():\n",
    "    summaries.append(get_experiment_summary(df, name))\n",
    "\n",
    "summary_df = pd.DataFrame(summaries)\n",
    "print(\"\\nüìä EXPERIMENT OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23e9ec",
   "metadata": {},
   "source": [
    "## 3. Pruning Analysis\n",
    "\n",
    "Analyze pruning rates and efficiency across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Pruning Analysis ===\n",
    "\n",
    "def analyze_pruning(experiment_data: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Create pruning analysis visualization.\"\"\"\n",
    "    pruning_stats = []\n",
    "    \n",
    "    for name, df in experiment_data.items():\n",
    "        total = len(df)\n",
    "        pruned = len(df[df['state'] == 'PRUNED'])\n",
    "        completed = len(df[df['state'] == 'COMPLETE'])\n",
    "        \n",
    "        pruning_stats.append({\n",
    "            'Experiment': name,\n",
    "            'Total': total,\n",
    "            'Completed': completed,\n",
    "            'Pruned': pruned,\n",
    "            'Pruning Rate (%)': pruned / total * 100 if total > 0 else 0\n",
    "        })\n",
    "    \n",
    "    pruning_df = pd.DataFrame(pruning_stats)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Stacked bar chart\n",
    "    x = range(len(pruning_df))\n",
    "    width = 0.6\n",
    "    \n",
    "    axes[0].bar(x, pruning_df['Completed'], width, label='Completed', color='#2ecc71')\n",
    "    axes[0].bar(x, pruning_df['Pruned'], width, bottom=pruning_df['Completed'], label='Pruned', color='#e74c3c')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(pruning_df['Experiment'], rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Number of Trials')\n",
    "    axes[0].set_title('Trial Outcomes by Experiment')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Pruning rate comparison\n",
    "    colors = [EXPERIMENT_COLORS.get(name, '#333333') for name in pruning_df['Experiment']]\n",
    "    axes[1].barh(pruning_df['Experiment'], pruning_df['Pruning Rate (%)'], color=colors)\n",
    "    axes[1].set_xlabel('Pruning Rate (%)')\n",
    "    axes[1].set_title('Pruning Rate by Experiment')\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    \n",
    "    for i, v in enumerate(pruning_df['Pruning Rate (%)']):\n",
    "        axes[1].text(v + 1, i, f'{v:.1f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pruning_df\n",
    "\n",
    "pruning_df = analyze_pruning(experiment_data)\n",
    "print(\"\\nüìä Pruning Statistics:\")\n",
    "display(pruning_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfe464",
   "metadata": {},
   "source": [
    "## 4. Best Model Comparison\n",
    "\n",
    "Compare the best models from each experiment using key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Best Model Comparison with Zoomed Charts ===\n",
    "\n",
    "def get_best_models(experiment_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Extract best model from each experiment.\"\"\"\n",
    "    best_models = []\n",
    "    \n",
    "    for name, df in experiment_data.items():\n",
    "        completed = df[df['state'] == 'COMPLETE']\n",
    "        if len(completed) == 0:\n",
    "            continue\n",
    "            \n",
    "        best_idx = completed['value'].idxmin()\n",
    "        best = completed.loc[best_idx]\n",
    "        \n",
    "        model_info = {\n",
    "            'Experiment': name,\n",
    "            'Trial': best.get('trial_number', 'N/A'),\n",
    "            'Val Loss': best['value'],\n",
    "            'Test MAE': best.get('test_mae'),\n",
    "            'Test RMSE': best.get('test_rmse'),\n",
    "            'Test MAPE': best.get('test_mape'),\n",
    "            'Dir Accuracy': best.get('test_directional_accuracy'),\n",
    "            'Lookback': best.get('lookback'),\n",
    "            'Batch Size': best.get('batch_size'),\n",
    "            'Hidden Size': best.get('hidden_size'),\n",
    "            'Attention Heads': best.get('attention_head_size'),\n",
    "            'LSTM Layers': best.get('lstm_layers'),\n",
    "            'Dropout': best.get('dropout'),\n",
    "            'Learning Rate': best.get('learning_rate')\n",
    "        }\n",
    "        best_models.append(model_info)\n",
    "    \n",
    "    return pd.DataFrame(best_models)\n",
    "\n",
    "best_models_df = get_best_models(experiment_data)\n",
    "print(\"\\nüèÜ BEST MODELS BY EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "display(best_models_df)\n",
    "\n",
    "# Visualization with zoomed axes\n",
    "if len(best_models_df) > 0:\n",
    "    metrics = ['Test MAE', 'Test RMSE', 'Test MAPE', 'Dir Accuracy']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in best_models_df.columns:\n",
    "            data = best_models_df[['Experiment', metric]].dropna()\n",
    "            if len(data) > 0:\n",
    "                colors = [EXPERIMENT_COLORS.get(exp, '#333333') for exp in data['Experiment']]\n",
    "                bars = axes[i].barh(data['Experiment'], data[metric], color=colors)\n",
    "                axes[i].set_xlabel(metric)\n",
    "                axes[i].set_title(f'{metric} by Experiment')\n",
    "                \n",
    "                # Zoom to data range for better visibility\n",
    "                data_min = data[metric].min()\n",
    "                data_max = data[metric].max()\n",
    "                data_range = data_max - data_min\n",
    "                \n",
    "                if data_range > 0 and data_min > 0:\n",
    "                    # Zoom: start from slightly below min\n",
    "                    zoom_min = max(0, data_min - data_range * 0.5)\n",
    "                    zoom_max = data_max + data_range * 0.2\n",
    "                    axes[i].set_xlim(zoom_min, zoom_max)\n",
    "                    axes[i].annotate('‚ö†Ô∏è Axis zoomed to show differences', \n",
    "                                     xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                                     fontsize=8, va='top', alpha=0.7)\n",
    "                \n",
    "                for bar, val in zip(bars, data[metric]):\n",
    "                    axes[i].text(bar.get_width() + (data_range * 0.02 if data_range > 0 else 0.01), \n",
    "                                bar.get_y() + bar.get_height()/2, \n",
    "                                f'{val:.4f}' if metric != 'Dir Accuracy' else f'{val:.2f}%',\n",
    "                                va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_results/tft_best_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03958b8",
   "metadata": {},
   "source": [
    "## 5. Relative Performance Comparison\n",
    "\n",
    "Show percentage differences from the best model for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Relative Performance Charts ===\n",
    "\n",
    "def plot_relative_performance(best_models_df: pd.DataFrame):\n",
    "    \"\"\"Show relative performance as % worse than best for each metric.\"\"\"\n",
    "    if len(best_models_df) == 0:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    metrics = ['Test MAE', 'Test RMSE', 'Test MAPE']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric not in best_models_df.columns:\n",
    "            continue\n",
    "            \n",
    "        data = best_models_df[['Experiment', metric]].dropna()\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate % worse than best (lower is better for these metrics)\n",
    "        best_val = data[metric].min()\n",
    "        data['Pct Worse'] = ((data[metric] - best_val) / best_val * 100).round(2)\n",
    "        \n",
    "        colors = [EXPERIMENT_COLORS.get(exp, '#333333') for exp in data['Experiment']]\n",
    "        bars = axes[i].barh(data['Experiment'], data['Pct Worse'], color=colors)\n",
    "        axes[i].set_xlabel('% Worse Than Best')\n",
    "        axes[i].set_title(f'{metric} - Relative Performance')\n",
    "        axes[i].axvline(x=0, color='green', linestyle='--', linewidth=2, label='Best')\n",
    "        \n",
    "        for bar, pct in zip(bars, data['Pct Worse']):\n",
    "            label = 'BEST' if pct == 0 else f'+{pct:.1f}%'\n",
    "            color = 'green' if pct == 0 else 'black'\n",
    "            axes[i].text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                        label, va='center', fontsize=9, color=color, fontweight='bold' if pct == 0 else 'normal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_results/tft_relative_performance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_relative_performance(best_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfaa977",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Analysis\n",
    "\n",
    "Analyze the distribution and impact of different hyperparameters across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06818db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Hyperparameter Distributions ===\n",
    "\n",
    "def plot_hyperparameter_distributions(experiment_data: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Plot distributions of key TFT hyperparameters.\"\"\"\n",
    "    # Combine all completed trials\n",
    "    all_completed = []\n",
    "    for name, df in experiment_data.items():\n",
    "        completed = df[df['state'] == 'COMPLETE'].copy()\n",
    "        completed['Experiment'] = name\n",
    "        all_completed.append(completed)\n",
    "    \n",
    "    if not all_completed:\n",
    "        print(\"No completed trials to analyze\")\n",
    "        return\n",
    "    \n",
    "    combined_df = pd.concat(all_completed, ignore_index=True)\n",
    "    \n",
    "    # TFT-specific hyperparameters to analyze\n",
    "    params_to_plot = ['lookback', 'hidden_size', 'lstm_layers', 'dropout', 'learning_rate', 'batch_size']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(params_to_plot):\n",
    "        if param in combined_df.columns:\n",
    "            data = combined_df[[param, 'Experiment']].dropna()\n",
    "            if len(data) > 0:\n",
    "                for exp_name in data['Experiment'].unique():\n",
    "                    exp_data = data[data['Experiment'] == exp_name][param]\n",
    "                    if len(exp_data) > 0:\n",
    "                        axes[i].hist(exp_data, alpha=0.5, label=exp_name, \n",
    "                                    color=EXPERIMENT_COLORS.get(exp_name, '#333333'),\n",
    "                                    bins=min(20, len(exp_data.unique())))\n",
    "                \n",
    "                axes[i].set_xlabel(param)\n",
    "                axes[i].set_ylabel('Count')\n",
    "                axes[i].set_title(f'{param} Distribution')\n",
    "                axes[i].legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_results/tft_hyperparameter_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_hyperparameter_distributions(experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb04075",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Impact Analysis\n",
    "\n",
    "Analyze how different hyperparameter values correlate with model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Hyperparameter Impact on Val Loss ===\n",
    "\n",
    "def analyze_param_impact(experiment_data: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Analyze impact of hyperparameters on validation loss.\"\"\"\n",
    "    # Combine all completed trials\n",
    "    all_completed = []\n",
    "    for name, df in experiment_data.items():\n",
    "        completed = df[df['state'] == 'COMPLETE'].copy()\n",
    "        completed['Experiment'] = name\n",
    "        all_completed.append(completed)\n",
    "    \n",
    "    if not all_completed:\n",
    "        print(\"No completed trials to analyze\")\n",
    "        return\n",
    "    \n",
    "    combined_df = pd.concat(all_completed, ignore_index=True)\n",
    "    \n",
    "    # Key TFT parameters to analyze\n",
    "    categorical_params = ['lookback', 'hidden_size', 'lstm_layers', 'batch_size']\n",
    "    continuous_params = ['dropout', 'learning_rate']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    # Box plots for categorical params\n",
    "    for param in categorical_params:\n",
    "        if param in combined_df.columns and plot_idx < len(axes):\n",
    "            data = combined_df[[param, 'value']].dropna()\n",
    "            if len(data) > 0:\n",
    "                # Group by param value and calculate mean val_loss\n",
    "                grouped = data.groupby(param)['value'].agg(['mean', 'std', 'count']).reset_index()\n",
    "                grouped = grouped.sort_values(param)\n",
    "                \n",
    "                axes[plot_idx].bar(grouped[param].astype(str), grouped['mean'], \n",
    "                                  yerr=grouped['std'], capsize=5, color='steelblue', alpha=0.7)\n",
    "                axes[plot_idx].set_xlabel(param)\n",
    "                axes[plot_idx].set_ylabel('Mean Val Loss')\n",
    "                axes[plot_idx].set_title(f'Val Loss by {param}')\n",
    "                axes[plot_idx].tick_params(axis='x', rotation=45)\n",
    "                plot_idx += 1\n",
    "    \n",
    "    # Scatter plots for continuous params\n",
    "    for param in continuous_params:\n",
    "        if param in combined_df.columns and plot_idx < len(axes):\n",
    "            data = combined_df[[param, 'value']].dropna()\n",
    "            if len(data) > 0:\n",
    "                axes[plot_idx].scatter(data[param], data['value'], alpha=0.3, s=10)\n",
    "                axes[plot_idx].set_xlabel(param)\n",
    "                axes[plot_idx].set_ylabel('Val Loss')\n",
    "                axes[plot_idx].set_title(f'Val Loss vs {param}')\n",
    "                \n",
    "                # Add trend line\n",
    "                try:\n",
    "                    z = np.polyfit(data[param], data['value'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_line = np.linspace(data[param].min(), data[param].max(), 100)\n",
    "                    axes[plot_idx].plot(x_line, p(x_line), 'r--', alpha=0.8, label='Trend')\n",
    "                    axes[plot_idx].legend()\n",
    "                except:\n",
    "                    pass\n",
    "                plot_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_results/tft_param_impact.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "analyze_param_impact(experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8be06",
   "metadata": {},
   "source": [
    "## 8. Cross-Experiment Optimal Parameters\n",
    "\n",
    "Find the optimal hyperparameters that work best across all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Cross-Experiment Optimal Parameters ===\n",
    "\n",
    "def find_optimal_params(experiment_data: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Find optimal hyperparameters across all experiments.\"\"\"\n",
    "    # Combine top N trials from each experiment\n",
    "    top_n = 10\n",
    "    all_top = []\n",
    "    \n",
    "    for name, df in experiment_data.items():\n",
    "        completed = df[df['state'] == 'COMPLETE'].copy()\n",
    "        if len(completed) > 0:\n",
    "            top_trials = completed.nsmallest(min(top_n, len(completed)), 'value')\n",
    "            top_trials['Experiment'] = name\n",
    "            all_top.append(top_trials)\n",
    "    \n",
    "    if not all_top:\n",
    "        print(\"No completed trials to analyze\")\n",
    "        return\n",
    "    \n",
    "    top_df = pd.concat(all_top, ignore_index=True)\n",
    "    \n",
    "    # Analyze most common parameter values in top models\n",
    "    params = ['lookback', 'hidden_size', 'lstm_layers', 'batch_size', 'attention_head_size']\n",
    "    \n",
    "    print(\"\\nüéØ OPTIMAL PARAMETER VALUES (from top performers)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    optimal_params = {}\n",
    "    for param in params:\n",
    "        if param in top_df.columns:\n",
    "            data = top_df[param].dropna()\n",
    "            if len(data) > 0:\n",
    "                most_common = data.mode().iloc[0] if len(data.mode()) > 0 else data.iloc[0]\n",
    "                mean_val = data.mean()\n",
    "                optimal_params[param] = most_common\n",
    "                print(f\"{param}: Most common = {most_common}, Mean = {mean_val:.2f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, len(params), figsize=(4*len(params), 4))\n",
    "    if len(params) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        if param in top_df.columns:\n",
    "            data = top_df[param].dropna()\n",
    "            if len(data) > 0:\n",
    "                value_counts = data.value_counts().sort_index()\n",
    "                axes[i].bar(value_counts.index.astype(str), value_counts.values, color='steelblue')\n",
    "                axes[i].set_xlabel(param)\n",
    "                axes[i].set_ylabel('Count in Top Models')\n",
    "                axes[i].set_title(f'{param} in Top {top_n} Models')\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_results/tft_optimal_params.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_params\n",
    "\n",
    "optimal_params = find_optimal_params(experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ca75c",
   "metadata": {},
   "source": [
    "## 9. Convergence Analysis\n",
    "\n",
    "Analyze how quickly each experiment converged to good solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322fb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Convergence Analysis ===\n",
    "\n",
    "def plot_convergence(experiment_data: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Plot convergence curves for each experiment.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for name, df in experiment_data.items():\n",
    "        completed = df[df['state'] == 'COMPLETE'].copy()\n",
    "        if len(completed) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Sort by trial number and compute cumulative best\n",
    "        completed = completed.sort_values('trial_number')\n",
    "        completed['cummin_val_loss'] = completed['value'].cummin()\n",
    "        \n",
    "        ax.plot(completed['trial_number'], completed['cummin_val_loss'], \n",
    "               label=name, color=EXPERIMENT_COLORS.get(name, '#333333'), linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Trial Number')\n",
    "    ax.set_ylabel('Best Val Loss So Far')\n",
    "    ax.set_title('Convergence Analysis - Best Val Loss Over Trials')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_results/tft_convergence.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_convergence(experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a9398",
   "metadata": {},
   "source": [
    "## 10. Final Summary & Recommendations\n",
    "\n",
    "Summary of findings and recommendations for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a52463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11: Final Summary ===\n",
    "\n",
    "def generate_final_summary(best_models_df: pd.DataFrame, experiment_data: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Generate final summary and recommendations.\"\"\"\n",
    "    if len(best_models_df) == 0:\n",
    "        print(\"No data available for summary\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Best overall model (by val loss)\n",
    "    best_val = best_models_df.loc[best_models_df['Val Loss'].idxmin()]\n",
    "    print(f\"\\nüèÜ Best by Validation Loss: {best_val['Experiment']}\")\n",
    "    print(f\"   Val Loss: {best_val['Val Loss']:.6f}\")\n",
    "    if pd.notna(best_val.get('Test MAE')):\n",
    "        print(f\"   Test MAE: {best_val['Test MAE']:.4f}\")\n",
    "    \n",
    "    # Best by test metrics\n",
    "    if 'Test MAE' in best_models_df.columns:\n",
    "        mae_valid = best_models_df.dropna(subset=['Test MAE'])\n",
    "        if len(mae_valid) > 0:\n",
    "            best_mae = mae_valid.loc[mae_valid['Test MAE'].idxmin()]\n",
    "            print(f\"\\nüéØ Best by Test MAE: {best_mae['Experiment']}\")\n",
    "            print(f\"   Test MAE: {best_mae['Test MAE']:.4f}\")\n",
    "    \n",
    "    if 'Dir Accuracy' in best_models_df.columns:\n",
    "        da_valid = best_models_df.dropna(subset=['Dir Accuracy'])\n",
    "        if len(da_valid) > 0:\n",
    "            best_da = da_valid.loc[da_valid['Dir Accuracy'].idxmax()]\n",
    "            print(f\"\\nüéØ Best by Directional Accuracy: {best_da['Experiment']}\")\n",
    "            print(f\"   Dir Accuracy: {best_da['Dir Accuracy']:.2f}%\")\n",
    "    \n",
    "    # Total trials analyzed\n",
    "    total_trials = sum(len(df) for df in experiment_data.values())\n",
    "    total_completed = sum(len(df[df['state'] == 'COMPLETE']) for df in experiment_data.values())\n",
    "    \n",
    "    print(f\"\\nüìà Total Trials Analyzed: {total_trials}\")\n",
    "    print(f\"   Completed: {total_completed} ({total_completed/total_trials*100:.1f}%)\")\n",
    "    print(f\"   Pruned: {total_trials - total_completed} ({(total_trials-total_completed)/total_trials*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Create summary table for export\n",
    "    return best_models_df\n",
    "\n",
    "final_summary = generate_final_summary(best_models_df, experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf6370",
   "metadata": {},
   "source": [
    "## 11. Radar Chart Comparison\n",
    "\n",
    "Visual comparison of best models using a radar/spider chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 12: Radar Chart ===\n",
    "\n",
    "def plot_radar_chart(best_models_df: pd.DataFrame):\n",
    "    \"\"\"Create radar chart comparing experiments.\"\"\"\n",
    "    metrics = ['Test MAE', 'Test RMSE', 'Test MAPE']\n",
    "    \n",
    "    # Filter to experiments with all metrics\n",
    "    valid_df = best_models_df.dropna(subset=metrics)\n",
    "    if len(valid_df) == 0:\n",
    "        print(\"Not enough data for radar chart\")\n",
    "        return\n",
    "    \n",
    "    # Normalize metrics (lower is better, so invert)\n",
    "    normalized = valid_df.copy()\n",
    "    for metric in metrics:\n",
    "        max_val = normalized[metric].max()\n",
    "        min_val = normalized[metric].min()\n",
    "        if max_val != min_val:\n",
    "            # Invert so higher is better on the chart\n",
    "            normalized[metric] = 1 - (normalized[metric] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized[metric] = 1\n",
    "    \n",
    "    # Add directional accuracy (higher is better, no inversion needed)\n",
    "    if 'Dir Accuracy' in valid_df.columns:\n",
    "        metrics.append('Dir Accuracy')\n",
    "        da_data = valid_df['Dir Accuracy'].dropna()\n",
    "        if len(da_data) > 0:\n",
    "            max_da = da_data.max()\n",
    "            min_da = da_data.min()\n",
    "            if max_da != min_da:\n",
    "                normalized['Dir Accuracy'] = (valid_df['Dir Accuracy'] - min_da) / (max_da - min_da)\n",
    "            else:\n",
    "                normalized['Dir Accuracy'] = 1\n",
    "    \n",
    "    # Create radar chart\n",
    "    num_vars = len(metrics)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    for _, row in normalized.iterrows():\n",
    "        values = [row[m] for m in metrics]\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, \n",
    "               label=row['Experiment'], color=EXPERIMENT_COLORS.get(row['Experiment'], '#333333'))\n",
    "        ax.fill(angles, values, alpha=0.1, color=EXPERIMENT_COLORS.get(row['Experiment'], '#333333'))\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_title('TFT Model Comparison (Higher = Better)', size=14, y=1.1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_results/tft_radar_chart.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_radar_chart(best_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8e5fe",
   "metadata": {},
   "source": [
    "## 12. Export Results\n",
    "\n",
    "Export summary tables and charts for the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964991e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 13: Export Results ===\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('saved_results/tft_analysis_output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export summary dataframe\n",
    "if len(summary_df) > 0:\n",
    "    summary_df.to_csv(output_dir / 'tft_experiment_summary.csv', index=False)\n",
    "    print(f\"‚úÖ Saved experiment summary to {output_dir / 'tft_experiment_summary.csv'}\")\n",
    "\n",
    "# Export best models dataframe\n",
    "if len(best_models_df) > 0:\n",
    "    best_models_df.to_csv(output_dir / 'tft_best_models.csv', index=False)\n",
    "    print(f\"‚úÖ Saved best models to {output_dir / 'tft_best_models.csv'}\")\n",
    "\n",
    "# Export pruning stats\n",
    "if len(pruning_df) > 0:\n",
    "    pruning_df.to_csv(output_dir / 'tft_pruning_stats.csv', index=False)\n",
    "    print(f\"‚úÖ Saved pruning stats to {output_dir / 'tft_pruning_stats.csv'}\")\n",
    "\n",
    "print(f\"\\nüìÅ All results exported to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
